{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * \n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"corpora\": {\n",
      "        \"semeval-2016-2017-task3-subtaskBC\": {\n",
      "            \"num_records\": -1,\n",
      "            \"record_format\": \"dict\",\n",
      "            \"file_size\": 6344358,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py\",\n",
      "            \"license\": \"All files released for the task are free for general research use\",\n",
      "            \"fields\": {\n",
      "                \"2016-train\": [\n",
      "                    \"...\"\n",
      "                ],\n",
      "                \"2016-dev\": [\n",
      "                    \"...\"\n",
      "                ],\n",
      "                \"2017-test\": [\n",
      "                    \"...\"\n",
      "                ],\n",
      "                \"2016-test\": [\n",
      "                    \"...\"\n",
      "                ]\n",
      "            },\n",
      "            \"description\": \"SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section \\u201cPapers\\u201d of https://github.com/RaRe-Technologies/gensim-data/issues/18.\",\n",
      "            \"checksum\": \"701ea67acd82e75f95e1d8e62fb0ad29\",\n",
      "            \"file_name\": \"semeval-2016-2017-task3-subtaskBC.gz\",\n",
      "            \"read_more\": [\n",
      "                \"http://alt.qcri.org/semeval2017/task3/\",\n",
      "                \"http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf\",\n",
      "                \"https://github.com/RaRe-Technologies/gensim-data/issues/18\",\n",
      "                \"https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english\"\n",
      "            ],\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"semeval-2016-2017-task3-subtaskA-unannotated\": {\n",
      "            \"num_records\": 189941,\n",
      "            \"record_format\": \"dict\",\n",
      "            \"file_size\": 234373151,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py\",\n",
      "            \"license\": \"These datasets are free for general research use.\",\n",
      "            \"fields\": {\n",
      "                \"THREAD_SEQUENCE\": \"\",\n",
      "                \"RelQuestion\": {\n",
      "                    \"RELQ_CATEGORY\": \"question category, according to the Qatar Living taxonomy\",\n",
      "                    \"RELQ_DATE\": \"date of posting\",\n",
      "                    \"RELQ_ID\": \"question indentifier\",\n",
      "                    \"RELQ_USERID\": \"identifier of the user asking the question\",\n",
      "                    \"RELQ_USERNAME\": \"name of the user asking the question\",\n",
      "                    \"RelQBody\": \"body of question\",\n",
      "                    \"RelQSubject\": \"subject of question\"\n",
      "                },\n",
      "                \"RelComments\": [\n",
      "                    {\n",
      "                        \"RelCText\": \"text of answer\",\n",
      "                        \"RELC_USERID\": \"identifier of the user posting the comment\",\n",
      "                        \"RELC_ID\": \"comment identifier\",\n",
      "                        \"RELC_USERNAME\": \"name of the user posting the comment\",\n",
      "                        \"RELC_DATE\": \"date of posting\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"description\": \"SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.\",\n",
      "            \"checksum\": \"2de0e2f2c4f91c66ae4fcf58d50ba816\",\n",
      "            \"file_name\": \"semeval-2016-2017-task3-subtaskA-unannotated.gz\",\n",
      "            \"read_more\": [\n",
      "                \"http://alt.qcri.org/semeval2016/task3/\",\n",
      "                \"http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf\",\n",
      "                \"https://github.com/RaRe-Technologies/gensim-data/issues/18\",\n",
      "                \"https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english\"\n",
      "            ],\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"patent-2017\": {\n",
      "            \"num_records\": 353197,\n",
      "            \"record_format\": \"dict\",\n",
      "            \"file_size\": 3087262469,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py\",\n",
      "            \"license\": \"not found\",\n",
      "            \"description\": \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
      "            \"checksum-0\": \"818501f0b9af62d3b88294d86d509f8f\",\n",
      "            \"checksum-1\": \"66c05635c1d3c7a19b4a335829d09ffa\",\n",
      "            \"file_name\": \"patent-2017.gz\",\n",
      "            \"read_more\": [\n",
      "                \"http://patents.reedtech.com/pgrbft.php\"\n",
      "            ],\n",
      "            \"parts\": 2\n",
      "        },\n",
      "        \"quora-duplicate-questions\": {\n",
      "            \"num_records\": 404290,\n",
      "            \"record_format\": \"dict\",\n",
      "            \"file_size\": 21684784,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py\",\n",
      "            \"license\": \"probably https://www.quora.com/about/tos\",\n",
      "            \"fields\": {\n",
      "                \"question1\": \"the full text of each question\",\n",
      "                \"question2\": \"the full text of each question\",\n",
      "                \"qid1\": \"unique ids of each question\",\n",
      "                \"qid2\": \"unique ids of each question\",\n",
      "                \"id\": \"the id of a training set question pair\",\n",
      "                \"is_duplicate\": \"the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise\"\n",
      "            },\n",
      "            \"description\": \"Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.\",\n",
      "            \"checksum\": \"d7cfa7fbc6e2ec71ab74c495586c6365\",\n",
      "            \"file_name\": \"quora-duplicate-questions.gz\",\n",
      "            \"read_more\": [\n",
      "                \"https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\"\n",
      "            ],\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"wiki-english-20171001\": {\n",
      "            \"num_records\": 4924894,\n",
      "            \"record_format\": \"dict\",\n",
      "            \"file_size\": 6516051717,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py\",\n",
      "            \"license\": \"https://dumps.wikimedia.org/legal.html\",\n",
      "            \"fields\": {\n",
      "                \"section_texts\": \"list of body of sections\",\n",
      "                \"section_titles\": \"list of titles of sections\",\n",
      "                \"title\": \"Title of wiki article\"\n",
      "            },\n",
      "            \"description\": \"Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`\",\n",
      "            \"checksum-0\": \"a7d7d7fd41ea7e2d7fa32ec1bb640d71\",\n",
      "            \"checksum-1\": \"b2683e3356ffbca3b6c2dca6e9801f9f\",\n",
      "            \"checksum-2\": \"c5cde2a9ae77b3c4ebce804f6df542c2\",\n",
      "            \"checksum-3\": \"00b71144ed5e3aeeb885de84f7452b81\",\n",
      "            \"file_name\": \"wiki-english-20171001.gz\",\n",
      "            \"read_more\": [\n",
      "                \"https://dumps.wikimedia.org/enwiki/20171001/\"\n",
      "            ],\n",
      "            \"parts\": 4\n",
      "        },\n",
      "        \"text8\": {\n",
      "            \"num_records\": 1701,\n",
      "            \"record_format\": \"list of str (tokens)\",\n",
      "            \"file_size\": 33182058,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py\",\n",
      "            \"license\": \"not found\",\n",
      "            \"description\": \"First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.\",\n",
      "            \"checksum\": \"68799af40b6bda07dfa47a32612e5364\",\n",
      "            \"file_name\": \"text8.gz\",\n",
      "            \"read_more\": [\n",
      "                \"http://mattmahoney.net/dc/textdata.html\"\n",
      "            ],\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"fake-news\": {\n",
      "            \"num_records\": 12999,\n",
      "            \"record_format\": \"dict\",\n",
      "            \"file_size\": 20102776,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py\",\n",
      "            \"license\": \"https://creativecommons.org/publicdomain/zero/1.0/\",\n",
      "            \"fields\": {\n",
      "                \"crawled\": \"date the story was archived\",\n",
      "                \"ord_in_thread\": \"\",\n",
      "                \"published\": \"date published\",\n",
      "                \"participants_count\": \"number of participants\",\n",
      "                \"shares\": \"number of Facebook shares\",\n",
      "                \"replies_count\": \"number of replies\",\n",
      "                \"main_img_url\": \"image from story\",\n",
      "                \"spam_score\": \"data from webhose.io\",\n",
      "                \"uuid\": \"unique identifier\",\n",
      "                \"language\": \"data from webhose.io\",\n",
      "                \"title\": \"title of story\",\n",
      "                \"country\": \"data from webhose.io\",\n",
      "                \"domain_rank\": \"data from webhose.io\",\n",
      "                \"author\": \"author of story\",\n",
      "                \"comments\": \"number of Facebook comments\",\n",
      "                \"site_url\": \"site URL from BS detector\",\n",
      "                \"text\": \"text of story\",\n",
      "                \"thread_title\": \"\",\n",
      "                \"type\": \"type of website (label from BS detector)\",\n",
      "                \"likes\": \"number of Facebook likes\"\n",
      "            },\n",
      "            \"description\": \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
      "            \"checksum\": \"5e64e942df13219465927f92dcefd5fe\",\n",
      "            \"file_name\": \"fake-news.gz\",\n",
      "            \"read_more\": [\n",
      "                \"https://www.kaggle.com/mrisdal/fake-news\"\n",
      "            ],\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"20-newsgroups\": {\n",
      "            \"num_records\": 18846,\n",
      "            \"record_format\": \"dict\",\n",
      "            \"file_size\": 14483581,\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py\",\n",
      "            \"license\": \"not found\",\n",
      "            \"fields\": {\n",
      "                \"topic\": \"name of topic (20 variant of possible values)\",\n",
      "                \"set\": \"marker of original split (possible values 'train' and 'test')\",\n",
      "                \"data\": \"\",\n",
      "                \"id\": \"original id inferred from folder name\"\n",
      "            },\n",
      "            \"description\": \"The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.\",\n",
      "            \"checksum\": \"c92fd4f6640a86d5ba89eaad818a9891\",\n",
      "            \"file_name\": \"20-newsgroups.gz\",\n",
      "            \"read_more\": [\n",
      "                \"http://qwone.com/~jason/20Newsgroups/\"\n",
      "            ],\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"__testing_matrix-synopsis\": {\n",
      "            \"description\": \"[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.\",\n",
      "            \"checksum\": \"1767ac93a089b43899d54944b07d9dc5\",\n",
      "            \"file_name\": \"__testing_matrix-synopsis.gz\",\n",
      "            \"read_more\": [\n",
      "                \"http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis\"\n",
      "            ],\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"__testing_multipart-matrix-synopsis\": {\n",
      "            \"description\": \"[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.\",\n",
      "            \"checksum-0\": \"c8b0c7d8cf562b1b632c262a173ac338\",\n",
      "            \"checksum-1\": \"5ff7fc6818e9a5d9bc1cf12c35ed8b96\",\n",
      "            \"checksum-2\": \"966db9d274d125beaac7987202076cba\",\n",
      "            \"file_name\": \"__testing_multipart-matrix-synopsis.gz\",\n",
      "            \"read_more\": [\n",
      "                \"http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis\"\n",
      "            ],\n",
      "            \"parts\": 3\n",
      "        }\n",
      "    },\n",
      "    \"models\": {\n",
      "        \"fasttext-wiki-news-subwords-300\": {\n",
      "            \"num_records\": 999999,\n",
      "            \"file_size\": 1005007116,\n",
      "            \"base_dataset\": \"Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py\",\n",
      "            \"license\": \"https://creativecommons.org/licenses/by-sa/3.0/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 300\n",
      "            },\n",
      "            \"description\": \"1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\",\n",
      "            \"read_more\": [\n",
      "                \"https://fasttext.cc/docs/en/english-vectors.html\",\n",
      "                \"https://arxiv.org/abs/1712.09405\",\n",
      "                \"https://arxiv.org/abs/1607.01759\"\n",
      "            ],\n",
      "            \"checksum\": \"de2bb3a20c46ce65c9c131e1ad9a77af\",\n",
      "            \"file_name\": \"fasttext-wiki-news-subwords-300.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"conceptnet-numberbatch-17-06-300\": {\n",
      "            \"num_records\": 1917247,\n",
      "            \"file_size\": 1225497562,\n",
      "            \"base_dataset\": \"ConceptNet, word2vec, GloVe, and OpenSubtitles 2016\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py\",\n",
      "            \"license\": \"https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 300\n",
      "            },\n",
      "            \"description\": \"ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\",\n",
      "            \"read_more\": [\n",
      "                \"http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972\",\n",
      "                \"https://github.com/commonsense/conceptnet-numberbatch\",\n",
      "                \"http://conceptnet.io/\"\n",
      "            ],\n",
      "            \"checksum\": \"fd642d457adcd0ea94da0cd21b150847\",\n",
      "            \"file_name\": \"conceptnet-numberbatch-17-06-300.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"word2vec-ruscorpora-300\": {\n",
      "            \"num_records\": 184973,\n",
      "            \"file_size\": 208427381,\n",
      "            \"base_dataset\": \"Russian National Corpus (about 250M words)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py\",\n",
      "            \"license\": \"https://creativecommons.org/licenses/by/4.0/deed.en\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 300,\n",
      "                \"window_size\": 10\n",
      "            },\n",
      "            \"description\": \"Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\",\n",
      "            \"preprocessing\": \"The corpus was lemmatized and tagged with Universal PoS\",\n",
      "            \"read_more\": [\n",
      "                \"https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models\",\n",
      "                \"http://rusvectores.org/en/\",\n",
      "                \"https://github.com/RaRe-Technologies/gensim-data/issues/3\"\n",
      "            ],\n",
      "            \"checksum\": \"9bdebdc8ae6d17d20839dd9b5af10bc4\",\n",
      "            \"file_name\": \"word2vec-ruscorpora-300.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"word2vec-google-news-300\": {\n",
      "            \"num_records\": 3000000,\n",
      "            \"file_size\": 1743563840,\n",
      "            \"base_dataset\": \"Google News (about 100 billion words)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py\",\n",
      "            \"license\": \"not found\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 300\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
      "            \"read_more\": [\n",
      "                \"https://code.google.com/archive/p/word2vec/\",\n",
      "                \"https://arxiv.org/abs/1301.3781\",\n",
      "                \"https://arxiv.org/abs/1310.4546\",\n",
      "                \"https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"a5e5354d40acb95f9ec66d5977d140ef\",\n",
      "            \"file_name\": \"word2vec-google-news-300.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-wiki-gigaword-50\": {\n",
      "            \"num_records\": 400000,\n",
      "            \"file_size\": 69182535,\n",
      "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 50\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"c289bc5d7f2f02c6dc9f2f9b67641813\",\n",
      "            \"file_name\": \"glove-wiki-gigaword-50.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-wiki-gigaword-100\": {\n",
      "            \"num_records\": 400000,\n",
      "            \"file_size\": 134300434,\n",
      "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 100\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"40ec481866001177b8cd4cb0df92924f\",\n",
      "            \"file_name\": \"glove-wiki-gigaword-100.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-wiki-gigaword-200\": {\n",
      "            \"num_records\": 400000,\n",
      "            \"file_size\": 264336934,\n",
      "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 200\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"59652db361b7a87ee73834a6c391dfc1\",\n",
      "            \"file_name\": \"glove-wiki-gigaword-200.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-wiki-gigaword-300\": {\n",
      "            \"num_records\": 400000,\n",
      "            \"file_size\": 394362229,\n",
      "            \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 300\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"29e9329ac2241937d55b852e8284e89b\",\n",
      "            \"file_name\": \"glove-wiki-gigaword-300.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-twitter-25\": {\n",
      "            \"num_records\": 1193514,\n",
      "            \"file_size\": 109885004,\n",
      "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 25\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"50db0211d7e7a2dcd362c6b774762793\",\n",
      "            \"file_name\": \"glove-twitter-25.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-twitter-50\": {\n",
      "            \"num_records\": 1193514,\n",
      "            \"file_size\": 209216938,\n",
      "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 50\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"c168f18641f8c8a00fe30984c4799b2b\",\n",
      "            \"file_name\": \"glove-twitter-50.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-twitter-100\": {\n",
      "            \"num_records\": 1193514,\n",
      "            \"file_size\": 405932991,\n",
      "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 100\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"b04f7bed38756d64cf55b58ce7e97b15\",\n",
      "            \"file_name\": \"glove-twitter-100.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"glove-twitter-200\": {\n",
      "            \"num_records\": 1193514,\n",
      "            \"file_size\": 795373100,\n",
      "            \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "            \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py\",\n",
      "            \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "            \"parameters\": {\n",
      "                \"dimension\": 200\n",
      "            },\n",
      "            \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.\",\n",
      "            \"read_more\": [\n",
      "                \"https://nlp.stanford.edu/projects/glove/\",\n",
      "                \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "            ],\n",
      "            \"checksum\": \"e52e8392d1860b95d5308a525817d8f9\",\n",
      "            \"file_name\": \"glove-twitter-200.gz\",\n",
      "            \"parts\": 1\n",
      "        },\n",
      "        \"__testing_word2vec-matrix-synopsis\": {\n",
      "            \"description\": \"[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.\",\n",
      "            \"parameters\": {\n",
      "                \"dimensions\": 50\n",
      "            },\n",
      "            \"preprocessing\": \"Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.\",\n",
      "            \"read_more\": [],\n",
      "            \"checksum\": \"534dcb8b56a360977a269b7bfc62d124\",\n",
      "            \"file_name\": \"__testing_word2vec-matrix-synopsis.gz\",\n",
      "            \"parts\": 1\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "info = api.info()\n",
    "print(json.dumps(info, indent=4))\n",
    "\n",
    "# Retry downloading the pre-trained Word2Vec model\n",
    "pretrained_model_path = api.load('word2vec-google-news-300', return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['dialogue_id', 'services', 'turns'],\n",
      "        num_rows: 8437\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['dialogue_id', 'services', 'turns'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['dialogue_id', 'services', 'turns'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "{'dialogue_id': 'PMUL4398.json', 'services': ['restaurant', 'hotel'], 'turns': {'turn_id': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], 'speaker': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], 'utterance': ['i need a place to dine in the center thats expensive', 'I have several options for you; do you prefer African, Asian, or British food?', 'Any sort of food would be fine, as long as it is a bit expensive. Could I get the phone number for your recommendation?', 'There is an Afrian place named Bedouin in the centre. How does that sound?', 'Sounds good, could I get that phone number? Also, could you recommend me an expensive hotel?', \"Bedouin's phone is 01223367660. As far as hotels go, I recommend the University Arms Hotel in the center of town.\", 'Yes. Can you book it for me?', 'Sure, when would you like that reservation?', 'i want to book it for 2 people and 2 nights starting from saturday.', 'Your booking was successful. Your reference number is FRGZWQL2 . May I help you further?', 'That is all I need to know. Thanks, good bye.', 'Thank you so much for Cambridge TownInfo centre. Have a great day!'], 'frames': [{'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': [], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': [], 'slots_values_list': []}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': ['restaurant-food'], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': [], 'slots_values_list': []}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': ['restaurant-phone'], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-name', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['bedouin'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-pricerange', 'hotel-type'], 'slots_values_list': [['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['hotel'], 'state': [{'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-name', 'hotel-pricerange', 'hotel-type'], 'slots_values_list': [['university arms hotel'], ['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['hotel'], 'state': [{'active_intent': 'book_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-bookday', 'hotel-bookpeople', 'hotel-bookstay', 'hotel-name', 'hotel-pricerange', 'hotel-type'], 'slots_values_list': [['saturday'], ['2'], ['2'], ['university arms hotel'], ['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': [], 'state': [], 'slots': []}, {'service': [], 'state': [], 'slots': []}], 'dialogue_acts': [{'dialog_act': {'act_type': ['Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'pricerange'], 'slot_value': ['centre', 'expensive']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform'], 'act_slot_name': ['area', 'pricerange'], 'act_slot_value': ['centre', 'expensive'], 'span_start': [30, 43], 'span_end': [36, 52]}}, {'dialog_act': {'act_type': ['Restaurant-Inform', 'Restaurant-Select'], 'act_slots': [{'slot_name': ['choice'], 'slot_value': ['several']}, {'slot_name': ['food', 'food', 'food'], 'slot_value': ['African', 'Asian', 'British']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Select', 'Restaurant-Select', 'Restaurant-Select'], 'act_slot_name': ['choice', 'food', 'food', 'food'], 'act_slot_value': ['several', 'African', 'Asian', 'British'], 'span_start': [7, 46, 55, 65], 'span_end': [14, 53, 60, 72]}}, {'dialog_act': {'act_type': ['Restaurant-Request'], 'act_slots': [{'slot_name': ['food'], 'slot_value': ['?']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'food', 'name'], 'slot_value': ['centre', 'Afrian', 'Bedouin']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform', 'Restaurant-Inform'], 'act_slot_name': ['food', 'name', 'area'], 'act_slot_value': ['Afrian', 'Bedouin', 'centre'], 'span_start': [12, 31, 46], 'span_end': [18, 38, 52]}}, {'dialog_act': {'act_type': ['Hotel-Inform', 'Restaurant-Request'], 'act_slots': [{'slot_name': ['pricerange', 'type'], 'slot_value': ['expensive', 'hotel']}, {'slot_name': ['phone'], 'slot_value': ['?']}]}, 'span_info': {'act_type': ['Hotel-Inform', 'Hotel-Inform'], 'act_slot_name': ['pricerange', 'type'], 'act_slot_value': ['expensive', 'hotel'], 'span_start': [76, 86], 'span_end': [85, 91]}}, {'dialog_act': {'act_type': ['Hotel-Recommend', 'Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'name'], 'slot_value': ['center of town', 'the University Arms Hotel']}, {'slot_name': ['name', 'phone'], 'slot_value': ['Bedouin', '01223367660']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform', 'Hotel-Recommend', 'Hotel-Recommend'], 'act_slot_name': ['name', 'phone', 'name', 'area'], 'act_slot_value': ['Bedouin', '01223367660', 'the University Arms Hotel', 'center of town'], 'span_start': [0, 19, 65, 98], 'span_end': [7, 30, 90, 112]}}, {'dialog_act': {'act_type': ['Hotel-Inform'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Booking-Request'], 'act_slots': [{'slot_name': ['bookday'], 'slot_value': ['?']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Hotel-Inform'], 'act_slots': [{'slot_name': ['bookday', 'bookpeople', 'bookstay'], 'slot_value': ['saturday', '2', '2']}]}, 'span_info': {'act_type': ['Hotel-Inform', 'Hotel-Inform', 'Hotel-Inform'], 'act_slot_name': ['bookstay', 'bookpeople', 'bookday'], 'act_slot_value': ['2', '2', 'saturday'], 'span_start': [22, 35, 58], 'span_end': [23, 36, 66]}}, {'dialog_act': {'act_type': ['Booking-Book', 'general-reqmore'], 'act_slots': [{'slot_name': ['ref'], 'slot_value': ['FRGZWQL2']}, {'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': ['Booking-Book'], 'act_slot_name': ['ref'], 'act_slot_value': ['FRGZWQL2'], 'span_start': [54], 'span_end': [62]}}, {'dialog_act': {'act_type': ['general-bye'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['general-bye', 'general-welcome'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}, {'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load MultiWOZ 2.2 dataset\n",
    "dataset = load_dataset(\"multi_woz_v22\")\n",
    "\n",
    "# Print available splits (train, validation, test)\n",
    "print(dataset)\n",
    "\n",
    "# View a sample dialogue\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hotel', 'train', 'taxi', 'bus', 'hospital', 'restaurant', 'attraction']\n"
     ]
    }
   ],
   "source": [
    "total_categories = list(set(element for sublist in dataset['train']['services'] for element in sublist))\n",
    "\n",
    "print(total_categories)\n",
    "\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"data/categories.json\", \"w\") as json_file:\n",
    "    json.dump(total_categories, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Load the downloaded model\n",
    "pretrained_model = KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_indices(vector1, vector2):\n",
    "    # Calculate the cosine similarity between each row in vector1 and vector2\n",
    "    similarities = cosine_similarity(vector1, vector2)\n",
    "\n",
    "    # For each row in vector1, find the index of the most similar row in vector2 \n",
    "    most_similar_indices = np.argmax(similarities, axis=1)\n",
    "    \n",
    "    return most_similar_indices\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def embed_sentence(sentence):\n",
    "    # Split the sentence into tokens\n",
    "    tokens = tokenize_sentence(sentence)\n",
    "    \n",
    "    # Initialize an empty array to store the word embeddings\n",
    "    embeddings = np.zeros(pretrained_model.vector_size)\n",
    "    \n",
    "    n = 0\n",
    "    # Iterate over each token in the sentence\n",
    "    for token in tokens:\n",
    "        # Check if the token is present in the pretrained word2vec model\n",
    "        if token in pretrained_model:\n",
    "            # Add the word embedding to the sentence embeddings\n",
    "            embeddings += pretrained_model[token]\n",
    "            n+=1\n",
    "    \n",
    "    # Normalize the sentence embeddings\n",
    "    embeddings /= n+1\n",
    "    \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.dataframes = {}\n",
    "        self.themes = total_categories\n",
    "        self.name = 'MarioBot'\n",
    "\n",
    "        self.presentation = 'Hi, my name is MarioBot, I am a chatbot. I am here to help you with any questions you may have regarding: '\n",
    "\n",
    "\n",
    "        for theme in self.themes:\n",
    "\n",
    "            temp_df = pd.read_csv(f\"data/{theme}.csv\")\n",
    "            self.dataframes[theme] = temp_df\n",
    "\n",
    "        self.nlp = None\n",
    "        self.w2v = None\n",
    "\n",
    "    def load_models(self):\n",
    "\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        pretrained_model_path = \"/Users/rocco02/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\"\n",
    "        self.w2v = KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)\n",
    "\n",
    "    \n",
    "    def get_dialogue(self):\n",
    "\n",
    "        print(f\"{self.name}: {self.presentation + ', '.join(self.themes)}.\\nPlease ask me a question:\")\n",
    "\n",
    "        self.load_models()\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            query = input(\"Insert the question: \")\n",
    "\n",
    "            print(f\"\\nUser: {query}\")\n",
    "\n",
    "            matching_words =  self.find_matching_words(query)\n",
    "\n",
    "            topic = matching_words[0]\n",
    "\n",
    "            if len(matching_words) > 1:\n",
    "                \n",
    "                print(f'\\n{self.name}: Looks like you are asking about multiple topics. We will solve one topic at time to avoid confusion. We start with the first topic: {matching_words[0]}.')\n",
    "\n",
    "            elif len(matching_words) == 0:\n",
    "\n",
    "                print(f\"\\n{self.name}: I am sorry, I do not have information about that topic. Try to rephrase the question or ask me something else.\")\n",
    "\n",
    "            \n",
    "            answer = self.find_best_answer(query, topic)\n",
    "            print(f\"\\n{self.name}: {answer}\")\n",
    "\n",
    "        \n",
    "    def tokenize_sentence(self, sentence):\n",
    "        \n",
    "        doc = self.nlp(sentence)\n",
    "        tokens = [token.text.lower() for token in doc if not token.is_punct]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def embed_sentence(self, sentence):\n",
    "        # Split the sentence into tokens\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        \n",
    "        # Initialize an empty array to store the word embeddings\n",
    "        embeddings = np.zeros(self.w2v.vector_size)\n",
    "        \n",
    "        n = 0\n",
    "        # Iterate over each token in the sentence\n",
    "        for token in tokens:\n",
    "            # Check if the token is present in the pretrained word2vec model\n",
    "            if token in self.w2v:\n",
    "                # Add the word embedding to the sentence embeddings\n",
    "                embeddings += self.w2v[token]\n",
    "                n+=1\n",
    "        \n",
    "        # Normalize the sentence embeddings\n",
    "        embeddings /= n+1\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def find_matching_words(self, query):\n",
    "        return [word for word in self.themes if word in query]\n",
    "    \n",
    "    def find_best_answer(self, query, topic):\n",
    "        \n",
    "        embed_query = embed_sentence(query)\n",
    "        df = self.dataframes[topic]['question_embeddings'].to_numpy()\n",
    "\n",
    "        most_similar_responce = calculate_similarity_indices(embed_query, df)\n",
    "\n",
    "        predicted_sentence = df.iloc[most_similar_responce[0]]['answer']\n",
    "\n",
    "        return predicted_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i need a place to dine in the center thats exp...</td>\n",
       "      <td>I have several options for you; do you prefer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any sort of food would be fine, as long as it ...</td>\n",
       "      <td>There is an Afrian place named Bedouin in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sounds good, could I get that phone number? Al...</td>\n",
       "      <td>Bedouin's phone is 01223367660. As far as hote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi there! Can you give me some info on Cityroomz?</td>\n",
       "      <td>Cityroomz is located at Sleeperz Hotel, Statio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes please. I need it for 7 people for 3 night...</td>\n",
       "      <td>How many days would you like to book it for?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  i need a place to dine in the center thats exp...   \n",
       "1  Any sort of food would be fine, as long as it ...   \n",
       "2  Sounds good, could I get that phone number? Al...   \n",
       "3  Hi there! Can you give me some info on Cityroomz?   \n",
       "4  Yes please. I need it for 7 people for 3 night...   \n",
       "\n",
       "                                              answer  \n",
       "0  I have several options for you; do you prefer ...  \n",
       "1  There is an Afrian place named Bedouin in the ...  \n",
       "2  Bedouin's phone is 01223367660. As far as hote...  \n",
       "3  Cityroomz is located at Sleeperz Hotel, Statio...  \n",
       "4       How many days would you like to book it for?  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = ChatBot()\n",
    "c.dataframes['restaurant'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRECOMPUTE THE DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hospital':                                               question  \\\n",
      "0                Could you find me a hospital in town?   \n",
      "1    Do they have a transitional care department? I...   \n",
      "2    I'm hurt and I need to find a hospital. Oh, an...   \n",
      "3       Thank you. Can I have the postcode too please.   \n",
      "4                  where is the Addenbrookes Hospital?   \n",
      "..                                                 ...   \n",
      "214                              I need the post code.   \n",
      "215  I've been injured and I need a hospital near m...   \n",
      "216  May I have the full hospital address and their...   \n",
      "217  I'm looking for the Addenbrookes Hospital and ...   \n",
      "218  Can you also tell me the postcode for the hosp...   \n",
      "\n",
      "                                                answer  \n",
      "0    Yes, Addenbrookes Hospital is in your area, wo...  \n",
      "1    Yes they have transitional care. Their post co...  \n",
      "2    Hello, I'm sorry you are injured. Addenbrookes...  \n",
      "3                               The postcode is CB20QQ  \n",
      "4    It's located at Hills Rd, Cambridge, you can r...  \n",
      "..                                                 ...  \n",
      "214  The hospital postcode is CB20QQ. Would you lik...  \n",
      "215  The closest hospital is Addenbrookes Hospital....  \n",
      "216  The hospital address is Hills Rd, Cambridge. T...  \n",
      "217  The phone number for Addenrookes Hospital, hae...  \n",
      "218  Yes, the postcode is CB20QQ. Can I get you any...  \n",
      "\n",
      "[219 rows x 2 columns], 'hotel':                                                 question  \\\n",
      "0      i need a place to dine in the center thats exp...   \n",
      "1      Any sort of food would be fine, as long as it ...   \n",
      "2      Sounds good, could I get that phone number? Al...   \n",
      "3      Guten Tag, I am staying overnight in Cambridge...   \n",
      "4      No, but I'd really like to be on the south end...   \n",
      "...                                                  ...   \n",
      "14256  Not in terms of that, but do they have free pa...   \n",
      "14257  Okay, I'd like to book a room at the Gonville ...   \n",
      "14258  Hi, can you help me find a moderately priced 3...   \n",
      "14259                            I do not need internet.   \n",
      "14260                   I don't, which do you recommend?   \n",
      "\n",
      "                                                  answer  \n",
      "0      I have several options for you; do you prefer ...  \n",
      "1      There is an Afrian place named Bedouin in the ...  \n",
      "2      Bedouin's phone is 01223367660. As far as hote...  \n",
      "3      I have 4 different options for you. I have two...  \n",
      "4      Sure. Does price matter? We can narrow it down...  \n",
      "...                                                  ...  \n",
      "14256  The Gonville hotel has 3 stars and parking, an...  \n",
      "14257  I'm sorry, there are no rooms available for th...  \n",
      "14258  I have 4 guesthouses with 3 stars that are mod...  \n",
      "14259  I have 2 in the north and 1 in both the south ...  \n",
      "14260  The hamilton lodge is in the north area and ha...  \n",
      "\n",
      "[14261 rows x 2 columns], 'taxi':                                                question  \\\n",
      "0     I am looking for a hotel named alyesbray lodge...   \n",
      "1     I would like to book a reservation for it. Can...   \n",
      "2     We will be arriving on Sunday and will stay fo...   \n",
      "3                               There will be 8 guests.   \n",
      "4                                 that is it thank you.   \n",
      "...                                                 ...   \n",
      "6408  Find me a modern european restaurant in the so...   \n",
      "6409                 Is it in the moderate price range?   \n",
      "6410  Yes, book for 7 people at 15:30 on friday please.   \n",
      "6411                         Well how about 14:30 then?   \n",
      "6412              do you know where castle galleries is   \n",
      "\n",
      "                                                 answer  \n",
      "0       i have their info, what would you like to know?  \n",
      "1     I sure can. First, I will need to know when yo...  \n",
      "2     Can you tell me how many guests will be stayin...  \n",
      "3     Booking was successful. Your reference number ...  \n",
      "4                             You have a nice day then!  \n",
      "...                                                 ...  \n",
      "6408  I found Restaurant alimentum that fits your ne...  \n",
      "6409   Yes it is. Would you like me to book it for you?  \n",
      "6410  I'm sorry, that isn't available for that time ...  \n",
      "6411  Yes, I was able to book your party at 14:30. Y...  \n",
      "6412  Yes, the address is unit su43, grande arcade, ...  \n",
      "\n",
      "[6413 rows x 2 columns], 'train':                                                 question  \\\n",
      "0            i need a train on tuesday out of kings lynn   \n",
      "1                            I want to leave on Tuesday.   \n",
      "2                          I'd like to leave after 9:30.   \n",
      "3      Looking for a train Tuesday leaving kings lynn...   \n",
      "4      No, I'd like a train leaving after 09:30 depar...   \n",
      "...                                                  ...   \n",
      "12076  Hi! I am looking for a train that arrives by 1...   \n",
      "12077  The train should depart from Bishops Stortford...   \n",
      "12078                   Yes please. I will need 5 seats.   \n",
      "12079  Great. I am also looking for suggestions on pl...   \n",
      "12080                   Do you have any multiple sports?   \n",
      "\n",
      "                                                  answer  \n",
      "0              What time of day would you like to leave?  \n",
      "1                Do you have a time you'd like to leave?  \n",
      "2                                 Is that 9:30 AM or PM?  \n",
      "3      I'm sorry, I'm not seeing any trains for that ...  \n",
      "4      There are no trains running at this time. Can ...  \n",
      "...                                                  ...  \n",
      "12076  I have 122 trains that are arriving by 11:30. ...  \n",
      "12077  TR2083 leaves Bishops Stortford at 09:29 on Fr...  \n",
      "12078  Ok, your booking is complete! The total is 50....  \n",
      "12079  I show 44 attractions in the centre area. Is t...  \n",
      "12080  There are no attractions matching that descrip...  \n",
      "\n",
      "[12081 rows x 2 columns], 'restaurant':                                                 question  \\\n",
      "0      i need a place to dine in the center thats exp...   \n",
      "1      Any sort of food would be fine, as long as it ...   \n",
      "2      Sounds good, could I get that phone number? Al...   \n",
      "3      Hi there! Can you give me some info on Cityroomz?   \n",
      "4      Yes please. I need it for 7 people for 3 night...   \n",
      "...                                                  ...   \n",
      "14600                   I would like Indian food please.   \n",
      "14601                            Is there anything else?   \n",
      "14602  Hello, I am looking for a cheap restaurant tha...   \n",
      "14603                    Yes, how about portuguese food?   \n",
      "14604                                 It doesn't matter.   \n",
      "\n",
      "                                                  answer  \n",
      "0      I have several options for you; do you prefer ...  \n",
      "1      There is an Afrian place named Bedouin in the ...  \n",
      "2      Bedouin's phone is 01223367660. As far as hote...  \n",
      "3      Cityroomz is located at Sleeperz Hotel, Statio...  \n",
      "4           How many days would you like to book it for?  \n",
      "...                                                  ...  \n",
      "14600  Sitar Tandoori is an expensive indian restaura...  \n",
      "14601  Pipasha Restaurant is in the expensive categor...  \n",
      "14602  It looks like there are no German restaurants ...  \n",
      "14603  There are two restaurants in the cheap price r...  \n",
      "14604  nandos serves portuguese food and in the cheap...  \n",
      "\n",
      "[14605 rows x 2 columns], 'bus':                                              question  \\\n",
      "0   Hello, I need to book a train at 07:15 leaving...   \n",
      "1                          Could you check for a bus?   \n",
      "2               I am leaving from Cambridge at 14:30.   \n",
      "3                             I will be going to ely.   \n",
      "4      Doesn't matter. I just can't leave until 14:30   \n",
      "5   I'm looking for a place called kambar, can you...   \n",
      "6   Can I get the postcode and phone number for th...   \n",
      "7   Thanks. I am also looking for a restaurant tha...   \n",
      "8                       Yes can you please book that?   \n",
      "9                              Is the Ali Baba cheap?   \n",
      "10  Well, then I really want to stay with somethin...   \n",
      "11  How much does it cost to take the bus in Cambr...   \n",
      "12                     I will be traveling to london.   \n",
      "13  I am also looking for a hotel in the west side...   \n",
      "14          I am looking for information in Cambridge   \n",
      "15     I'm looking for a hotel called, Archway House.   \n",
      "16  Alright. I'd like a room on saturday for 4 nig...   \n",
      "17  I will also need a train for Saturday, leaving...   \n",
      "18                                Going to Cambridge.   \n",
      "19  How much does it cost to take the bus in Cambr...   \n",
      "20                     I will be traveling to london.   \n",
      "21  I am also looking for a hotel in the west side...   \n",
      "22  I want a hotel on the west side that doesn't h...   \n",
      "23  Are you sure that there are no hotels on the w...   \n",
      "24       No price preference, whatever you recommend.   \n",
      "25  Can you help me find a train that leaves Cambr...   \n",
      "26  I am going to Bishops Storford on Wednesday. I...   \n",
      "27  That's disappointing. Can you recommend a taxi...   \n",
      "28  Okay. Can you also tell me about a museum to g...   \n",
      "\n",
      "                                               answer  \n",
      "0   Sure I can help with that. There are no trains...  \n",
      "1   I think we may need to recheck your intended a...  \n",
      "2                            And where are you going?  \n",
      "3                 What time do you want to arrive by?  \n",
      "4   There is a 15:50 train that will arrive at 16:...  \n",
      "5   Sure! It is a nightclub located in the centre ...  \n",
      "6   Certainly! Their phone number is 01223842725 a...  \n",
      "7   The ali baba is a very nice one. Should I book...  \n",
      "8                   How many people and for what day?  \n",
      "9   It is in the moderate price range. Would you l...  \n",
      "10  I recommend kohinoor, they are indian with a c...  \n",
      "11                    Where will you be traveling to?  \n",
      "12  The rate for a trip from Cambridge to london k...  \n",
      "13  There are a few to choose from. Do you prefer ...  \n",
      "14    What type of information do you need help with?  \n",
      "15  It is in the moderate price range at 52 gilber...  \n",
      "16  Booking was successful.Reference number is : S...  \n",
      "17  Sure, I can help you with that. Where would yo...  \n",
      "18                  Where will you be departing from?  \n",
      "19                    Where will you be traveling to?  \n",
      "20  The rate for a trip from Cambridge to london k...  \n",
      "21  There are a few to choose from. Do you prefer ...  \n",
      "22  Unfortunately there are no hotels meeting your...  \n",
      "23  There are four. Do you have a price range in m...  \n",
      "24  Finches bed and breakfast is a cheap guesthous...  \n",
      "25  I can help with that. What is the destination ...  \n",
      "26  I do not have any trains that match your request.  \n",
      "27  I can set you up for bus TR9984, they leave at...  \n",
      "28  Of course, there are actually eleven to choose...  , 'attraction':                                                 question  \\\n",
      "0      I'm looking for a places to go and see during ...   \n",
      "1           I would like to go to the south area please.   \n",
      "2      That's a lot of choices, how about nightclubs?...   \n",
      "3      Yes, I would like an expensive hotel with free...   \n",
      "4            Yeah, could you recommend a good gastropub?   \n",
      "...                                                  ...   \n",
      "10948  Hi! I am looking for a train that arrives by 1...   \n",
      "10949  The train should depart from Bishops Stortford...   \n",
      "10950                   Yes please. I will need 5 seats.   \n",
      "10951  Great. I am also looking for suggestions on pl...   \n",
      "10952                   Do you have any multiple sports?   \n",
      "\n",
      "                                                  answer  \n",
      "0                      What area of town would you like?  \n",
      "1      On the south side, your options are the cinema...  \n",
      "2      The only nightclub in the south is called the ...  \n",
      "3      I have places that fit that description all ov...  \n",
      "4      Backstreet Bistro. It's expensive though. Ther...  \n",
      "...                                                  ...  \n",
      "10948  I have 122 trains that are arriving by 11:30. ...  \n",
      "10949  TR2083 leaves Bishops Stortford at 09:29 on Fr...  \n",
      "10950  Ok, your booking is complete! The total is 50....  \n",
      "10951  I show 44 attractions in the centre area. Is t...  \n",
      "10952  There are no attractions matching that descrip...  \n",
      "\n",
      "[10953 rows x 2 columns]}\n"
     ]
    }
   ],
   "source": [
    "all_df = {}\n",
    "\n",
    "for df_name in total_categories:\n",
    "    all_df[df_name] = pd.read_csv(f\"data/{df_name}.csv\")\n",
    "\n",
    "print(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sentence('ciao a tutti').dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hospital.csv\n",
      "Saved hotel.csv\n",
      "Saved taxi.csv\n",
      "Saved train.csv\n",
      "Saved restaurant.csv\n",
      "Saved bus.csv\n",
      "Saved attraction.csv\n"
     ]
    }
   ],
   "source": [
    "for df_name, df in all_df.items():\n",
    "    # df['question_embeddings'] = df['question'].apply(embed_sentence)\n",
    "    # df.to_csv(f\"word2vec_data/{df_name}.csv\", index=False)\n",
    "    # df['question_embeddings'] = df['question'].apply(lambda x: \" \".join(map(str, embed_sentence(x))))\n",
    "    df.to_csv(f\"word2vec_data/{df_name}.csv\", index=False)\n",
    "    np.save(f\"word2vec_data/{df_name}_embeddings.npy\", np.vstack(df['question'].apply(embed_sentence)))\n",
    "    print(f\"Saved {df_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0              Could you find me a hospital in town?   \n",
      "1  Do they have a transitional care department? I...   \n",
      "2  I'm hurt and I need to find a hospital. Oh, an...   \n",
      "3     Thank you. Can I have the postcode too please.   \n",
      "4                where is the Addenbrookes Hospital?   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Yes, Addenbrookes Hospital is in your area, wo...   \n",
      "1  Yes they have transitional care. Their post co...   \n",
      "2  Hello, I'm sorry you are injured. Addenbrookes...   \n",
      "3                             The postcode is CB20QQ   \n",
      "4  It's located at Hills Rd, Cambridge, you can r...   \n",
      "\n",
      "                                 question_embeddings  \n",
      "0  0.0694427490234375 0.03167724609375 0.03784179...  \n",
      "1  -0.021839141845703125 0.022182464599609375 0.0...  \n",
      "2  -0.04683467320033482 0.024152483258928572 0.03...  \n",
      "3  -0.0166259765625 0.0154541015625 0.01789550781...  \n",
      "4  0.000445556640625 0.017529296875 0.07338867187...  \n",
      "                                            question  \\\n",
      "0  i need a place to dine in the center thats exp...   \n",
      "1  Any sort of food would be fine, as long as it ...   \n",
      "2  Sounds good, could I get that phone number? Al...   \n",
      "3  Guten Tag, I am staying overnight in Cambridge...   \n",
      "4  No, but I'd really like to be on the south end...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  I have several options for you; do you prefer ...   \n",
      "1  There is an Afrian place named Bedouin in the ...   \n",
      "2  Bedouin's phone is 01223367660. As far as hote...   \n",
      "3  I have 4 different options for you. I have two...   \n",
      "4  Sure. Does price matter? We can narrow it down...   \n",
      "\n",
      "                                 question_embeddings  \n",
      "0  -0.05179665305397727 0.08053311434659091 0.010...  \n",
      "1  0.007572339928668478 0.00912400950556216 0.012...  \n",
      "2  0.04067454618566176 -0.009467629825367647 0.01...  \n",
      "3  -0.019323012408088234 -0.004938013413373162 -0...  \n",
      "4  0.0057098388671875 0.0253265380859375 0.029227...  \n",
      "                                            question  \\\n",
      "0  I am looking for a hotel named alyesbray lodge...   \n",
      "1  I would like to book a reservation for it. Can...   \n",
      "2  We will be arriving on Sunday and will stay fo...   \n",
      "3                            There will be 8 guests.   \n",
      "4                              that is it thank you.   \n",
      "\n",
      "                                              answer  \\\n",
      "0    i have their info, what would you like to know?   \n",
      "1  I sure can. First, I will need to know when yo...   \n",
      "2  Can you tell me how many guests will be stayin...   \n",
      "3  Booking was successful. Your reference number ...   \n",
      "4                          You have a nice day then!   \n",
      "\n",
      "                                 question_embeddings  \n",
      "0  -0.023126220703125 0.0291748046875 0.033813476...  \n",
      "1  0.037933349609375 -0.015283993312290736 0.0604...  \n",
      "2  -0.0025736490885416665 0.040859222412109375 0....  \n",
      "3  0.000732421875 0.029317220052083332 0.03238932...  \n",
      "4  0.005467732747395833 -0.041481335957845054 0.0...  \n",
      "                                            question  \\\n",
      "0        i need a train on tuesday out of kings lynn   \n",
      "1                        I want to leave on Tuesday.   \n",
      "2                      I'd like to leave after 9:30.   \n",
      "3  Looking for a train Tuesday leaving kings lynn...   \n",
      "4  No, I'd like a train leaving after 09:30 depar...   \n",
      "\n",
      "                                              answer  \\\n",
      "0          What time of day would you like to leave?   \n",
      "1            Do you have a time you'd like to leave?   \n",
      "2                             Is that 9:30 AM or PM?   \n",
      "3  I'm sorry, I'm not seeing any trains for that ...   \n",
      "4  There are no trains running at this time. Can ...   \n",
      "\n",
      "                                 question_embeddings  \n",
      "0  -0.06122165256076389 0.0087890625 0.0222614076...  \n",
      "1  -0.016377766927083332 0.01739501953125 0.07287...  \n",
      "2  0.042154947916666664 0.022115071614583332 0.06...  \n",
      "3  -0.032945112748579544 0.053716486150568184 0.0...  \n",
      "4  0.015894571940104168 0.059231705135769315 0.03...  \n",
      "                                            question  \\\n",
      "0  i need a place to dine in the center thats exp...   \n",
      "1  Any sort of food would be fine, as long as it ...   \n",
      "2  Sounds good, could I get that phone number? Al...   \n",
      "3  Hi there! Can you give me some info on Cityroomz?   \n",
      "4  Yes please. I need it for 7 people for 3 night...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  I have several options for you; do you prefer ...   \n",
      "1  There is an Afrian place named Bedouin in the ...   \n",
      "2  Bedouin's phone is 01223367660. As far as hote...   \n",
      "3  Cityroomz is located at Sleeperz Hotel, Statio...   \n",
      "4       How many days would you like to book it for?   \n",
      "\n",
      "                                 question_embeddings  \n",
      "0  -0.05179665305397727 0.08053311434659091 0.010...  \n",
      "1  0.007572339928668478 0.00912400950556216 0.012...  \n",
      "2  0.04067454618566176 -0.009467629825367647 0.01...  \n",
      "3  0.061474609375 -0.01820068359375 0.03469772338...  \n",
      "4  0.02364501953125 -0.002017339070638021 0.02896...  \n",
      "                                            question  \\\n",
      "0  Hello, I need to book a train at 07:15 leaving...   \n",
      "1                         Could you check for a bus?   \n",
      "2              I am leaving from Cambridge at 14:30.   \n",
      "3                            I will be going to ely.   \n",
      "4     Doesn't matter. I just can't leave until 14:30   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Sure I can help with that. There are no trains...   \n",
      "1  I think we may need to recheck your intended a...   \n",
      "2                           And where are you going?   \n",
      "3                What time do you want to arrive by?   \n",
      "4  There is a 15:50 train that will arrive at 16:...   \n",
      "\n",
      "                                 question_embeddings  \n",
      "0  0.017454359266493056 0.038842095269097224 0.03...  \n",
      "1  0.038279215494791664 -0.029673258463541668 0.0...  \n",
      "2  -0.05266462053571429 -0.0009578977312360491 0....  \n",
      "3  -0.07503255208333333 0.06795247395833333 0.068...  \n",
      "4  0.016107177734375 0.0937286376953125 -0.043896...  \n",
      "                                            question  \\\n",
      "0  I'm looking for a places to go and see during ...   \n",
      "1       I would like to go to the south area please.   \n",
      "2  That's a lot of choices, how about nightclubs?...   \n",
      "3  Yes, I would like an expensive hotel with free...   \n",
      "4        Yeah, could you recommend a good gastropub?   \n",
      "\n",
      "                                              answer  \\\n",
      "0                  What area of town would you like?   \n",
      "1  On the south side, your options are the cinema...   \n",
      "2  The only nightclub in the south is called the ...   \n",
      "3  I have places that fit that description all ov...   \n",
      "4  Backstreet Bistro. It's expensive though. Ther...   \n",
      "\n",
      "                                 question_embeddings  \n",
      "0  0.014624962439903846 0.06963172325721154 0.027...  \n",
      "1  -0.0050048828125 0.053900824652777776 0.064325...  \n",
      "2  0.0708349609375 0.009618263244628906 -0.001606...  \n",
      "3  0.03298117897727273 0.027130820534446022 0.017...  \n",
      "4  0.014927455357142858 -0.04680524553571429 0.07...  \n"
     ]
    }
   ],
   "source": [
    "for df_name in total_categories:\n",
    "\n",
    "    a = pd.read_csv(f\"word2vec_data/{df_name}.csv\")\n",
    "    print(a.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot import ChatBot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pretrained_model_path = api.load('word2vec-google-news-300', return_path=True)\n",
    "w2v = KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "from utils import * \n",
    "import json\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "\n",
    "    def __init__(self, nlp, w2v):\n",
    "        \n",
    "        self.dataframes = {}\n",
    "        self.themes = None\n",
    "        self.name = 'MarioBot'\n",
    "        self.selected_theme = None\n",
    "        self.theme_embeddings = []\n",
    "        self.presentation = 'Hi, my name is MarioBot, I am a chatbot. I am here to help you with any questions you may have regarding: '\n",
    "\n",
    "        self.current_embedding = None\n",
    "        self.nlp = nlp\n",
    "        self.w2v = w2v\n",
    "\n",
    "        self.load_models()\n",
    "        \n",
    "    def load_models(self):\n",
    "\n",
    "        with open(\"data/categories.json\", \"r\") as json_file:\n",
    "            self.themes = json.load(json_file)\n",
    "\n",
    "        for theme in self.themes:\n",
    "\n",
    "            temp_df = pd.read_csv(f\"word2vec_data/{theme}.csv\")\n",
    "            self.dataframes[theme] = temp_df\n",
    "            self.theme_embeddings.append(w2v[theme])\n",
    "            \n",
    "    def get_dialogue(self):\n",
    "\n",
    "        print(f\"{self.name}: {self.presentation + ', '.join(self.themes)}.\\nPlease ask me a question:\")\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            query = input(\"Insert the question: \")\n",
    "            if query == '':\n",
    "                break\n",
    "            \n",
    "            print(f\"\\nUser: {query}\")\n",
    "\n",
    "            # if self.selected_theme is None:\n",
    "            #     matching_words =  self.find_matching_words(query)\n",
    "\n",
    "\n",
    "            #     if len(matching_words) > 1:\n",
    "            #         print(f'\\n{self.name}: Looks like you are asking about multiple topics. We will solve one topic at time to avoid confusion. We start with the first topic: {matching_words[0]}.')\n",
    "\n",
    "            #     elif len(matching_words) == 0:\n",
    "\n",
    "            #         print(f\"\\n{self.name}: I am sorry, I do not have information about that topic. Try to rephrase the question or ask me something else.\")\n",
    "            #         continue\n",
    "            \n",
    "            #     self.selected_theme = matching_words[0]\n",
    "            if self.selected_theme is None:\n",
    "                self.selected_theme = self.themes[np.argmax(cosine_similarity(embed_sentence(query).reshape(1, -1), self.theme_embeddings))]\n",
    "                print(f'\\nConversation Theme: {self.selected_theme}\\n')\n",
    "            answer = self.find_best_answer(query, self.selected_theme)\n",
    "            print(f\"\\n{self.name}: {answer}\")\n",
    "\n",
    "        \n",
    "    def tokenize_sentence(self, sentence):\n",
    "        \n",
    "        doc = self.nlp(sentence)\n",
    "        print(doc)\n",
    "        tokens = [token.text.lower() for token in doc if not token.is_punct]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def embed_sentence(self, sentence):\n",
    "        # Split the sentence into tokens\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        print(tokens)\n",
    "        # Initialize an empty array to store the word embeddings\n",
    "        embeddings = np.zeros(self.w2v.vector_size)\n",
    "        \n",
    "        n = 0\n",
    "        # Iterate over each token in the sentence\n",
    "        for token in tokens:\n",
    "            # Check if the token is present in the pretrained word2vec model\n",
    "            if token in self.w2v:\n",
    "                # Add the word embedding to the sentence embeddings\n",
    "                embeddings += self.w2v[token]\n",
    "                n+=1\n",
    "        \n",
    "        # Normalize the sentence embeddings\n",
    "        embeddings /= n+1\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def find_matching_words(self, query):\n",
    "        return [word for word in self.themes if word in query]\n",
    "    \n",
    "    def find_best_answer(self, query, topic):\n",
    "        \n",
    "        embed_query = embed_sentence(query).reshape(1, -1)\n",
    "        if self.current_embedding is None:\n",
    "            self.current_embedding = embed_query\n",
    "        \n",
    "        #df = pd.read_csv(f\"word2vec_data/{topic}.csv\")\n",
    "        df_embeddings = np.load(f\"word2vec_data/{topic}_embeddings.npy\")\n",
    "\n",
    "        most_similar_responce = calculate_similarity_indices(embed_query , df_embeddings)\n",
    "\n",
    "        predicted_sentence = self.dataframes[self.selected_theme].iloc[most_similar_responce[0]]['answer']\n",
    "        self.current_embedding = embed_sentence(predicted_sentence).reshape(1, -1) + embed_query + 0.3 * self.current_embedding\n",
    "        #print('Closest Words to Embedding:\\n')\n",
    "        #print(self.w2v.similar_by_vector(self.current_embedding.flatten(), topn=5))\n",
    "\n",
    "        return predicted_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to book a table for 2 people in a restaurant.\n",
    "c = ChatBot(nlp, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarioBot: Hi, my name is MarioBot, I am a chatbot. I am here to help you with any questions you may have regarding: hotel, train, taxi, bus, hospital, restaurant, attraction.\n",
      "Please ask me a question:\n",
      "\n",
      "User: hello hello hello place to get drinks\n",
      "\n",
      "Conversation Theme: taxi\n",
      "\n",
      "[[0.3600318  0.42872538 0.38493297 ... 0.40376772 0.26648418 0.4159486 ]]\n",
      "\n",
      "MarioBot: When would you like the taxi to pick you up?\n"
     ]
    }
   ],
   "source": [
    "c.get_dialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restaurant', 'attraction']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.find_matching_words('restaurants attractions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hotel', 'train', 'taxi', 'bus', 'hospital', 'restaurant', 'attraction']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('word2vec_data/attraction.csv')\n",
    "f = df['question_embeddings'].apply(lambda x: np.array(x.split(), dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10953,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '[-5.17966531e-02'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_best_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mI need a restaurant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrestaurant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mChatBot.find_best_answer\u001b[39m\u001b[34m(self, query, topic)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_best_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, topic):\n\u001b[32m    107\u001b[39m     embed_query = embed_sentence(query).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion_embeddings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     most_similar_responce = calculate_similarity_indices(embed_query , df)\n\u001b[32m    113\u001b[39m     predicted_sentence = df.iloc[most_similar_responce[\u001b[32m0\u001b[39m]][\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dialogue_system/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dialogue_system/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dialogue_system/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dialogue_system/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dialogue_system/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mChatBot.find_best_answer.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_best_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, topic):\n\u001b[32m    107\u001b[39m     embed_query = embed_sentence(query).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     df = \u001b[38;5;28mself\u001b[39m.dataframes[topic][\u001b[33m'\u001b[39m\u001b[33mquestion_embeddings\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    111\u001b[39m     most_similar_responce = calculate_similarity_indices(embed_query , df)\n\u001b[32m    113\u001b[39m     predicted_sentence = df.iloc[most_similar_responce[\u001b[32m0\u001b[39m]][\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '[-5.17966531e-02'"
     ]
    }
   ],
   "source": [
    "c.find_best_answer('I need a restaurant', 'restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('word2vec_data/hospital_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 300)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = w2v['Hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words = w2v.similar_by_vector(embedding, topn=1)  # Returns top 1 match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ciao', 1.0000001192092896)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "designai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
